{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from dython import nominal\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# linear-regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# decision dependencies\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Kmeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# scatterplot dependencies\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# embedding umap\n",
    "import umap\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Kmodes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "# LGBMclassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from typing import List, Dict, Union, Any\n",
    "sb.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset()\n",
    "- input: path to dataset folder\n",
    "- returns: dataframe with datatypes formatted \n",
    "- description: Function will do the following tasks:\n",
    "    - load the dataset (in this case Course_info.csv)\n",
    "    - load the yaml to format the features to the appropiate dataypes (in this case data_types.yaml)\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder_path: str) -> pd.DataFrame:\n",
    "\n",
    "    dataset: pd.DataFrame = pd.read_csv(f\"{folder_path}/Course_info.csv\")\n",
    "    config: Dict[str, str] = yaml.safe_load(open(f\"{folder_path}/data_types.yaml\"))\n",
    "    \n",
    "    # assign the data types preset in the data_types.yaml\n",
    "    for feature, datatype in config['data_types'].items():\n",
    "        dataset[feature] = dataset[feature].astype(datatype)\n",
    "\n",
    "    # for loop used to debug to ensure features are in the right data type\n",
    "    # for column in dataset.columns:\n",
    "    #     print(f\"{column}: {dataset[column].dtype}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_parameters()\n",
    "- input: path of yaml file\n",
    "- returns: parameters for the \n",
    "- description: will load the following parameters (in dictionary format):\n",
    "    - include_features\n",
    "    - Clean up\n",
    "    - Types of EDA\n",
    "    - \n",
    "- remarks: parameter file should contain the following details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parameters(folder_path: str) -> Dict[str, Union[List, Dict[str, str]]]:\n",
    "\n",
    "    parameters : Dict[str, Union[List, Dict[str, str]]] = yaml.safe_load(open(f\"{folder_path}/parameters.yaml\"))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean_up_dataset()\n",
    "- input: \n",
    "    - Dataframe\n",
    "    - parameters \n",
    "- returns: Dataframe\n",
    "- description: Function does the following to the dataset\n",
    "    - clean up \n",
    "    - drop row that contains NULL/NAN values\n",
    "    - extract the interested features\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required for the clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_dataset(\n",
    "        dataframe: pd.DataFrame, \n",
    "        parameters: Dict[str, Union[List, Dict[str, str]]]\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    ## CLEAN UP RELATED\n",
    "    # remove of rows based on constraint values set in the parameters \n",
    "    cleanup_parameters: Dict[str, int] = parameters['clean_up']\n",
    "    \n",
    "    for feature, constraint in cleanup_parameters.items():\n",
    "        dataframe = dataframe[ dataframe[feature] > constraint]\n",
    "\n",
    "    ## NULL/NAN RELATED\n",
    "    # to print any null values\n",
    "    # print(dataframe.isnull().sum())\n",
    "\n",
    "    # drop any row that contains NULL/NAN values\n",
    "    if dataframe.isnull().values.any():\n",
    "        dataframe = dataframe.dropna()\n",
    "\n",
    "    ## DUPLICATE RELATED\n",
    "    # method to print duplicates on specific column\n",
    "    # print(f\"Duplicated instructors: \\n{dataframe['instructor_name'].value_counts(ascending=False)}\")\n",
    "    # instructor_names = pd.DataFrame(dataframe['instructor_name'].value_counts(ascending=False))\n",
    "    # instructor_names.to_csv('instructor_names.csv')\n",
    "\n",
    "    # method to print duplicates exists on specific column\n",
    "    # for columns in dataframe.columns:\n",
    "    #     print(f\"Duplicated {columns}: {dataframe[columns].duplicated().any()}\")\n",
    "\n",
    "    # total_duplicated_ids = dataframe[dataframe.duplicated('id', keep=False)]\n",
    "    # print(f\"Course with duplicated ids: {len(total_duplicated_ids)}\")\n",
    "    # add condition if there is duplicates\n",
    "\n",
    "    ## EXTRACT FEATURE RELATED\n",
    "    # create a new dataframe to extract the interested feature (set in the parameters)\n",
    "    include_features: List[str] = parameters['include_features']\n",
    "    extracted_dataset = pd.DataFrame( dataframe[include_features] )\n",
    "    # extracted_dataset.info()\n",
    "\n",
    "    return extracted_dataset.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_outlier_samples()\n",
    "- inputs: Dataframe\n",
    "- returns: Series of outliers based on supplied dataframe\n",
    "- description: identify the outliers based on the supplied dataframe\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_samples(dataframe: pd.DataFrame) -> pd.core.series.Series:\n",
    "\n",
    "    q1 = dataframe.quantile(0.25)\n",
    "    q3 = dataframe.quantile(0.75)\n",
    "    interquartile_range = q3-q1\n",
    "\n",
    "    lower_whisker = q1-1.5*interquartile_range\n",
    "    upper_whisker = q3+1.5*interquartile_range\n",
    "    outliers: pd.core.series.Series = ((dataframe < lower_whisker) | (dataframe > upper_whisker))\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_feature_outliers()\n",
    "- inputs: Dataframe\n",
    "- returns: None\n",
    "- description: prints number of outliers for every numerical features/column\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_outliers(dataframe: pd.DataFrame) -> None:\n",
    "\n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    for column in numerical_dataframe.columns:\n",
    "\n",
    "        outliers = sum(get_outlier_samples(numerical_dataframe[column]))\n",
    "        print(f\"[{column}] total outliers: {outliers}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_outliers()\n",
    "- inputs: Dataframe\n",
    "- returns: Dataframe with outliers removed for every numerical feature/column \n",
    "- description: Function remove **UNION** outlier of the dataset. In other words remove the entire row containing outliers\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    union_outliers = (get_outlier_samples(numerical_dataframe)).any(axis=1)\n",
    "    dataframe_with_outliers_removed: pd.DataFrame = dataframe[~union_outliers].reset_index(drop=True)\n",
    "\n",
    "    # print(f\"Total 'UNION' outliers: {sum(union_outliers)}\")\n",
    "    # instructor_names = pd.DataFrame(dataframe_with_outliers_removed['instructor_name'].value_counts(ascending=False))\n",
    "    # instructor_names.to_csv('instructor_names.csv')\n",
    "    return dataframe_with_outliers_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_eda_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - List of catergorical features/columns which are numerical types\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates box, histo and violin plot for every numerical features (column) of the dataset\n",
    "- remakrs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_eda_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "    \n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "    total_features = len(numerical_dataframe.columns)\n",
    "    figure, axes = plt.subplots(\n",
    "        total_features, \n",
    "        3, \n",
    "        figsize=(24,4.8*total_features)\n",
    "    )\n",
    "    \n",
    "    # figure.suptitle(plot_title, fontsize=20)\n",
    "    axes[0, 1].set_title(plot_title, fontsize=25)\n",
    "    row = 0\n",
    "    for column in numerical_dataframe.columns:\n",
    "        sb.boxplot(data=numerical_dataframe[column], orient='h', ax=axes[row,0])\n",
    "        sb.histplot(data=numerical_dataframe[column], ax=axes[row,1])\n",
    "        sb.violinplot(data=numerical_dataframe[column], orient='h', ax=axes[row,2])\n",
    "        row = row + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_categorical_count_visualization()\n",
    "- input: \n",
    "    - Dataframe\n",
    "    - parameters \n",
    "- returns: None\n",
    "- description: generates categorical plot for selected catergorical features (column) of the dataset\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required for the catergorical count visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_count_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        parameters: Dict[str, Union[List, Dict[str, str]]]\n",
    "    ) -> None:\n",
    "\n",
    "    visualization_parameters: List[str] = parameters['categorical_count_visualization']\n",
    "    visualization_dataframe = pd.DataFrame( dataframe[visualization_parameters] )\n",
    "\n",
    "    for column in visualization_dataframe.columns:\n",
    "        category_total_types = len(dataframe[column].value_counts())        \n",
    "        sb.catplot(y=column, data=visualization_dataframe, kind=\"count\", height=category_total_types)\n",
    "        plt.title(f\"Total count for all types available in '{column}' feature\", fontsize=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_vs_categorical_eda_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - parameters\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates categorical plot for every catergorical features (column) of the dataset\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required to generate eda visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_vs_categorical_eda_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        parameters: Dict[str, Union[List, Dict[str, str]]],\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "\n",
    "    visualization_parameters: Dict[str, str] = parameters['numerical_vs_categorical_eda_visualization']\n",
    "\n",
    "    total_features = len(visualization_parameters)\n",
    "\n",
    "    figure, axes = plt.subplots(\n",
    "        total_features, \n",
    "        1, \n",
    "        figsize=(20,10*total_features),\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    figure.tight_layout(pad=10.0)\n",
    "\n",
    "    row = 0\n",
    "    for numerical_feature, categorical_feature in visualization_parameters.items():\n",
    "        axes[row].set_title(\n",
    "            f\"{categorical_feature} boxplot based on {numerical_feature}\", \n",
    "            fontdict={'fontsize': 25, 'fontweight': 'medium'}\n",
    "        )\n",
    "\n",
    "        sb_plot = sb.boxplot(\n",
    "            y=numerical_feature, \n",
    "            x=categorical_feature, \n",
    "            data=dataframe, \n",
    "            order=dataframe.groupby(categorical_feature)[numerical_feature].median().sort_values().index,\n",
    "            ax=axes[row]\n",
    "        )\n",
    "        sb_plot.set_xticklabels(sb_plot.get_xticklabels(), rotation=40, ha='right')\n",
    "\n",
    "        row = row + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_heatmap()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates heatmap for every numerical features (column) of the dataset\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_heatmap(\n",
    "        dataframe: pd.DataFrame,\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "    \n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(plot_title, fontsize=20)\n",
    "    sb.heatmap(numerical_dataframe.corr(), vmin = -1, vmax = 1, linewidths = 1,\n",
    "        annot = True, fmt = \".2f\", annot_kws = {\"size\": 18}, cmap = \"RdBu\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_categorical_heatmap()\n",
    "- inputs: dataframe\n",
    "- returns: None\n",
    "- description: generates heatmap for numerical and catergorical features (column) of the dataset\n",
    " heatmap produces by calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: \n",
    "    - Pearson's R for continuous-continuous cases \n",
    "    - Correlation Ratio for categorical-continuous cases \n",
    "    - Cramer's V or Theil's U for categorical-categorical cases\n",
    "- remarks: More info on the library checkout [dython](http://shakedzy.xyz/dython/modules/nominal/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_categorical_heatmap(\n",
    "    dataframe: pd.DataFrame\n",
    ") -> None:\n",
    "    nominal.associations(dataset=dataframe, figsize=(13, 10), title=\"Correlation/Strength-of-association of features\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_categorical_scatter_plot()\n",
    "- inputs: dataframe\n",
    "- returns: None\n",
    "- description: generates heatmap for numerical and catergorical features (column) of the dataset\n",
    " heatmap produces by calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: \n",
    "    - Pearson's R for continuous-continuous cases \n",
    "    - Correlation Ratio for categorical-continuous cases \n",
    "    - Cramer's V or Theil's U for categorical-categorical cases\n",
    "- remarks: More info on the library checkout [dython](http://shakedzy.xyz/dython/modules/nominal/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_categorical_scatter_plot(\n",
    "        dataset: pd.DataFrame,\n",
    "        parameters: Dict[str, Union[List[str], Dict[str, Any]]]\n",
    "    ) -> None:\n",
    "\n",
    "    kmodes_parameters: Dict[str, Any] = parameters['kmodes']\n",
    "\n",
    "    categorical_features: List[str] = kmodes_parameters['categorical_features']\n",
    "    numerical_features: List[str] = kmodes_parameters['numerical_features']\n",
    "    include_features: List[str] = categorical_features + numerical_features\n",
    "    print(f\"Include features: {include_features}\")\n",
    "\n",
    "    extracted_dataset = pd.DataFrame( dataset[include_features] )\n",
    "\n",
    "    # preprocessing numerical\n",
    "    # numerical_dataset = extracted_dataset.select_dtypes(exclude='category')\n",
    "    \n",
    "    # for column in numerical_dataset.columns:\n",
    "    #     power_transformer = PowerTransformer()\n",
    "    #     numerical_dataset.loc[:, column] = power_transformer.fit_transform(\n",
    "    #         np.array(numerical_dataset[column]).reshape(-1, 1)\n",
    "    #     )\n",
    "    \n",
    "    # print(\"HERE1\")\n",
    "    # # preprocessing categorical\n",
    "    # categorical_dataset = extracted_dataset.select_dtypes(include='category')\n",
    "    # categorical_dataset = pd.get_dummies(categorical_dataset)\n",
    "\n",
    "    # print(\"HERE2\")\n",
    "    # # percentage of columns which are categorical is used as weight parameter in embeddings later\n",
    "    # categorical_weight = len(extracted_dataset.select_dtypes(include='category').columns) / extracted_dataset.shape[1]\n",
    "\n",
    "    # print(\"HERE3\")\n",
    "    # # Embedding numerical & categorical\n",
    "    # fit1 = umap.UMAP(metric='l2').fit(numerical_dataset)\n",
    "    # fit2 = umap.UMAP(metric='dice').fit(categorical_dataset)\n",
    "\n",
    "    # print(\"HERE4\")\n",
    "    # # Augmenting the numerical embedding with categorical\n",
    "    # intersection = umap.umap_.general_simplicial_set_intersection(\n",
    "    #     fit1.graph_, \n",
    "    #     fit2.graph_, \n",
    "    #     weight=categorical_weight\n",
    "    # )\n",
    "\n",
    "    # print(\"HERE5\")\n",
    "    # intersection = umap.umap_.reset_local_connectivity(intersection)\n",
    "    # embedding = umap.umap_.simplicial_set_embedding(\n",
    "    #     fit1._raw_data, \n",
    "    #     intersection, \n",
    "    #     fit1.n_components, \n",
    "    #     fit1._initial_alpha, \n",
    "    #     fit1._a, \n",
    "    #     fit1._b, \n",
    "    #     fit1.repulsion_strength, \n",
    "    #     fit1.negative_sample_rate, \n",
    "    #     200, \n",
    "    #     'random', \n",
    "    #     np.random, \n",
    "    #     fit1.metric, \n",
    "    #     fit1._metric_kwds, \n",
    "    #     False\n",
    "    # )\n",
    "    \n",
    "    # plt.figure(figsize=(20, 10))\n",
    "    # plt.scatter(*embedding.T, s=2, cmap='Spectral', alpha=1.0)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # ax = sb.scatterplot(x=\"num_subscribers\", y=\"price\", data=extracted_dataset)\n",
    "    # ax.set_title(\"num_subscribers vs. price\")\n",
    "    sb.lmplot(x=\"num_subscribers\", y=\"price\", hue=\"instructor_name\", data=extracted_dataset)\n",
    "    # ax.set_xlabel(\"Fly ash\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_kprototype_clusters()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - numerical_feature: the numerical feature to associate with different type of x and y axis\n",
    "- returns: Dataframe with clusters\n",
    "- description: using of kprototype to generate cluster based on catergorical features\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required for kprototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kprototype_clusters(\n",
    "        dataset: pd.DataFrame, \n",
    "        parameters: Dict[str, Union[List[str], Dict[str, Any]]]\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "#     dataset.info()\n",
    "\n",
    "#     decision_tree_parameters: Dict[str, int] = parameters['multivariate_decision_tree']\n",
    "#     include_features: List[str] = decision_tree_parameters['predictors']\n",
    "#     include_features.append(decision_tree_parameters['response'])\n",
    "#     extracted_dataset = pd.DataFrame( dataset[include_features] )\n",
    "#     extracted_dataset.to_csv('extracted_dataset.csv')\n",
    "#     extracted_dataset.info()\n",
    "\n",
    "#     new_dataset = extracted_dataset.groupby(['instructor_name'], as_index=False).mean()\n",
    "#     new_column = extracted_dataset.groupby(['instructor_name']).size().reset_index(name='Number of courses')\n",
    "#     new_dataset['Number of courses'] = new_column['Number of courses']\n",
    "#     new_dataset.info()\n",
    "\n",
    "\n",
    "#     # create classess based on range\n",
    "#     # labels=['0<=10', '11<=50', '51<=100', '101<=150', '151 and above']\n",
    "#     labels=['small', 'medium', 'large', 'x-large', 'super']\n",
    "#     bins=[0,10,50,100,150, np.inf]\n",
    "\n",
    "# #     labels=['0<=10', '11<=20', '21<=30', '31<=40', '41<=50','51<=60', '61<=70', '71<=80', '81<=90', '91<=100', '101 and above']\n",
    "# #     bins=[0,10,20,30,40,50,60,70,80,90,100, np.inf]\n",
    "\n",
    "#     new_dataset['Class'] = pd.cut(new_dataset['Number of courses'], bins=bins, labels=labels)\n",
    "#     # dataset['Class'] = dataset['Class'].astype('category')\n",
    "#     new_dataset.to_csv('add_class_dataset.csv')\n",
    "#     include_features.pop()\n",
    "\n",
    "    # plt.show()    \n",
    "\n",
    "    ## KMODES\n",
    "    kmodes_parameters: Dict[str, Any] = parameters['kmodes']\n",
    "\n",
    "    categorical_features: List[str] = kmodes_parameters['categorical_features']\n",
    "    numerical_features: List[str] = kmodes_parameters['numerical_features']\n",
    "    include_features: List[str] = categorical_features + numerical_features\n",
    "    extracted_dataset = pd.DataFrame( dataset[include_features] )\n",
    "\n",
    "\n",
    "    # get the index of catergorical features from the dataset\n",
    "    categorical_features_idx = list()\n",
    "    for index, column in enumerate(extracted_dataset.columns):\n",
    "        if extracted_dataset[column].dtype == 'category':\n",
    "            categorical_features_idx.append(index)\n",
    "\n",
    "    mark_array=extracted_dataset.values\n",
    "    # print(f\"{mark_array}\")\n",
    "    kproto = KPrototypes(\n",
    "        n_clusters=10, \n",
    "        n_init=1, \n",
    "        verbose=2\n",
    "    ).fit(mark_array, categorical=categorical_features_idx)\n",
    "    print(f\"kproto type: {kproto}\")\n",
    "    print(kproto.cluster_centroids_)\n",
    "\n",
    "    clusters = kproto.predict(mark_array, categorical=categorical_features_idx)\n",
    "    extracted_dataset['Cluster'] = list(clusters)\n",
    "    extracted_dataset['Cluster'] = extracted_dataset['Cluster'].astype('category')\n",
    "\n",
    "    print(extracted_dataset.groupby('Cluster').size())\n",
    "    extracted_dataset.to_csv('k-proto.csv')\n",
    "    kproto_cost = kproto.cost_\n",
    "    kproto_label = kproto.labels_\n",
    "\n",
    "    print(f\"kproto cost type: {type(kproto_cost)}\")\n",
    "    print(f\"kproto label type: {type(kproto_label)}\")\n",
    "\n",
    "    plt.plot()\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show\n",
    "\n",
    "    # taking an hour to create\n",
    "    # costs = []\n",
    "    # n_clusters = []\n",
    "    # clusters_assigned = []\n",
    "\n",
    "    # for i in tqdm(range(0, 5)):\n",
    "    #     try:\n",
    "    #         kproto = KPrototypes(n_clusters=i, init='Huang', verbose=2)\n",
    "    #         clusters = kproto.fit_predict(\n",
    "    #             mark_array, \n",
    "    #             categorical=categorical_features_idx\n",
    "    #         )\n",
    "    #         costs.append(kproto.cost_)\n",
    "    #         n_clusters.append(i)\n",
    "    #         clusters_assigned.append(clusters)\n",
    "    #     except:\n",
    "    #         print(f\"Can't cluster with {i} clusters\")\n",
    "    \n",
    "    # plt.plot(costs)\n",
    "    # plt.xlabel('K')\n",
    "    # plt.ylabel('cost')\n",
    "    # plt.show\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    # fig.set_size_inches((20, 10))\n",
    "    # scatter = ax.scatter(embedding[:, 0], embedding[:, 1], s=2, c=clusters, cmap='tab20b', alpha=1.0)\n",
    "\n",
    "    # # produce a legend with the unique colors from the scatter\n",
    "    # legend1 = ax.legend(*scatter.legend_elements(num=15),\n",
    "    #                     loc=\"lower left\", title=\"Classes\")\n",
    "    # ax.add_artist(legend1)\n",
    "\n",
    "    return extracted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_pairplot(parameters: Dict[str, Union[List[str], Dict[str, Any]]]) -> None:\n",
    "def generate_pairplot() -> None:\n",
    "\n",
    "    dataset = pd.read_csv('k-proto.csv')\n",
    "\n",
    "    # kmodes_parameters: Dict[str, Any] = parameters['kmodes']\n",
    "    # numerical_features: List[str] = kmodes_parameters['numerical_features']\n",
    "    # total_features = len(numerical_features)\n",
    "\n",
    "    # figure, axes = plt.subplots(\n",
    "    #     total_features, \n",
    "    #     1, \n",
    "    #     figsize=(20,10*total_features),\n",
    "    #     constrained_layout=True\n",
    "    # )\n",
    "\n",
    "\n",
    "    f, axes = plt.subplots(1, 1, figsize=(18, 24))\n",
    "    sb.swarmplot(x = \"price\", y = \"Cluster\", data = dataset, orient = \"h\", ax = axes[0])\n",
    "\n",
    "\n",
    "    # count = 0\n",
    "    # for column in dataset.columns:\n",
    "\n",
    "    #     if dataset[column].dtype == 'category':\n",
    "    #         continue\n",
    "\n",
    "    #     sb.swarmplot(x = column, y = \"Cluster\", data = dataset, orient = \"h\", ax = axes[count])\n",
    "    #     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kproto_heatmap() -> None:\n",
    "\n",
    "    dataset = pd.read_csv('k-proto.csv')\n",
    "    nominal.associations(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix() -> None:\n",
    "    dataset = pd.read_csv('k-proto.csv')\n",
    "\n",
    "    y = pd.DataFrame(dataset[['Cluster']])\n",
    "    X = pd.DataFrame(dataset[['num_subscribers']])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    dectree = DecisionTreeClassifier(max_depth = 3)\n",
    "    dectree.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = dectree.predict(X_train)\n",
    "    y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "    print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "    print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "    print()\n",
    "\n",
    "    print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "    print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "    print()\n",
    "\n",
    "    f, axes = plt.subplots(1, 2, figsize=(40, 20))\n",
    "    sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "            annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "    sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "            annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree() -> None:\n",
    "    dataset = pd.read_csv('k-proto.csv')\n",
    "\n",
    "    y = pd.DataFrame(dataset[['Cluster']])\n",
    "    X = pd.DataFrame(dataset[['num_subscribers']])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    dectree = DecisionTreeClassifier(max_depth = 4)\n",
    "    dectree.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = dectree.predict(X_train)\n",
    "    y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "    f = plt.figure(figsize=(70,10))\n",
    "    plot_tree(dectree, filled=True, rounded=True, fontsize=10,\n",
    "        feature_names=X_train.columns, \n",
    "        class_names=[\"0\",\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "    plt.savefig('tree_high_dpi', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc1015",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
